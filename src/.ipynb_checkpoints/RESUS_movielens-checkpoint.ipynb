{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torch import transpose as t\n",
    "from torch import inverse as inv\n",
    "from torch import mm,solve,matmul\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic settings and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "dataset = 'ml-1m'\n",
    "PATH = f'../data/'\n",
    "COLD_USER_THRESHOLD = 30\n",
    "batch_size = 1024\n",
    "embedding_dim = 10\n",
    "device = torch.device('cuda:0')\n",
    "lr = 1e-3\n",
    "num_epochs = 100\n",
    "overfit_patience = 2\n",
    "exp_id=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=pd.read_csv(PATH+'train_df.csv')\n",
    "val_df = pd.read_csv(PATH+f'valid_df.csv')\n",
    "test_df = pd.read_csv(PATH+f'test_df.csv')\n",
    "\n",
    "# dataframe->pytorch dataset\n",
    "train_dataset = CTR_Dataset(train_df)\n",
    "val_dataset = CTR_Dataset(val_df)\n",
    "test_dataset = CTR_Dataset(test_df)\n",
    "num_fields = train_dataset.num_fields\n",
    "num_features = 1+max([x.x_id.max().item() for x in [train_dataset, val_dataset, test_dataset]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_support = val_df.groupby('uid',as_index=False).apply(lambda x: x[:COLD_USER_THRESHOLD] if len(x)>COLD_USER_THRESHOLD else x[:-1])\n",
    "val_df_query = val_df.groupby('uid',as_index=False).apply(lambda x: x[COLD_USER_THRESHOLD:] if len(x)>COLD_USER_THRESHOLD else x[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_support.to_csv('../data/val_df_support.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df_query.to_csv('../data/val_df_query.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,COLD_USER_THRESHOLD+1):\n",
    "    test_support_set = test_df.groupby('uid',as_index=False).apply(\n",
    "        lambda x: x[:i] if len(x)>i else x[:0])\n",
    "    test_query_set = test_df.groupby('uid',as_index=False).apply(\n",
    "        lambda x: x[i:] if len(x)>i else x[:0])\n",
    "    test_support_set.to_csv(f'../data/test/test_df_support_{i}.csv', index=False)\n",
    "    test_query_set.to_csv(f'../data/test/test_df_query_{i}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset class and some helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTR_Dataset(Dataset):\n",
    "    def __init__(self, data_df):\n",
    "        data_x_arr = data_df.drop(columns=['is_click']).values\n",
    "        self.num_fields = data_x_arr.shape[1]//2 - 1\n",
    "        self.x_id = torch.LongTensor(data_x_arr[:,1:self.num_fields+1])\n",
    "        self.x_value = torch.Tensor(data_x_arr[:,self.num_fields+2:])\n",
    "        self.y = torch.Tensor(data_df['is_click'].values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_id[idx], self.x_value[idx], self.y[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_id.shape[0]\n",
    "\n",
    "class QueryWithSupportDataset(Dataset):\n",
    "    def __init__(self, data_df, train_support_df, COLD_USER_THRESHOLD):\n",
    "        self.data_x_arr = data_df.drop(columns=['is_click']).values\n",
    "        self.num_fields = self.data_x_arr.shape[1]//2-1\n",
    "        self.x_id = torch.LongTensor(self.data_x_arr[:,1:self.num_fields+1])\n",
    "        self.x_value = torch.Tensor(self.data_x_arr[:,self.num_fields+2:])\n",
    "        self.y = torch.Tensor(data_df['is_click'].values)\n",
    "        self.train_support_df = train_support_df\n",
    "        self.COLD_USER_THRESHOLD = COLD_USER_THRESHOLD\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        uid=self.data_x_arr[idx][0].item()\n",
    "        df = self.train_support_df[self.train_support_df['uid']==uid]\n",
    "        data_x_arr = df.drop(columns=['is_click']).values\n",
    "        x_id_support_arr = data_x_arr[:,1:self.num_fields+1]\n",
    "        x_val_support_arr = data_x_arr[:,self.num_fields+2:]\n",
    "        y_support_arr = df['is_click'].values\n",
    "        if x_id_support_arr.shape[0]<self.COLD_USER_THRESHOLD:\n",
    "            x_id_support_arr_paddding = np.array([[0]*self.num_fields]*(\n",
    "                self.COLD_USER_THRESHOLD-x_id_support_arr[:self.COLD_USER_THRESHOLD].shape[0]))\n",
    "            x_id_support_arr = np.concatenate([x_id_support_arr,x_id_support_arr_paddding],axis=0)\n",
    "            x_val_support_arr_paddding = np.array([[0]*self.num_fields]*(\n",
    "                self.COLD_USER_THRESHOLD-x_val_support_arr[:self.COLD_USER_THRESHOLD].shape[0]))\n",
    "            x_val_support_arr = np.concatenate([x_val_support_arr,x_val_support_arr_paddding],axis=0)\n",
    "            y_support_arr_padding =  np.array([-1]*(\n",
    "                self.COLD_USER_THRESHOLD-y_support_arr[:self.COLD_USER_THRESHOLD].shape[0]))\n",
    "            y_support_arr = np.concatenate([y_support_arr,y_support_arr_padding],axis=0)\n",
    "        x_id_support = torch.LongTensor(x_id_support_arr)\n",
    "        x_val_support = torch.Tensor(x_val_support_arr)\n",
    "        y_support = torch.Tensor(y_support_arr)\n",
    "        return self.x_id[idx], self.x_value[idx], self.y[idx], [x_id_support,x_val_support,y_support]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x_id.shape[0]\n",
    "\n",
    "def val(model, val_dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    pred_arr = np.array([])\n",
    "    label_arr = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for itr, batch in tqdm(enumerate(val_dataloader)):\n",
    "            batch = [[e.to(device) for e in item] if isinstance(item, list) else item.to(device) for item in batch]\n",
    "            feature_ids, feature_vals, labels = batch\n",
    "            outputs = model(feature_ids, feature_vals).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            pred_arr = np.hstack(\n",
    "                [pred_arr, outputs.data.detach().cpu()]) if pred_arr.size else outputs.data.detach().cpu()\n",
    "            label_arr = np.hstack(\n",
    "                [label_arr, labels.data.detach().cpu()]) if label_arr.size else labels.data.detach().cpu()\n",
    "        val_loss = running_loss / (itr + 1)\n",
    "        torch.cuda.empty_cache()\n",
    "    auc = roc_auc_score(label_arr, pred_arr)\n",
    "    return val_loss, auc\n",
    "\n",
    "def val_query(model, val_dataloader):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    pred_arr = np.array([])\n",
    "    label_arr = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for itr, batch in enumerate(tqdm(val_dataloader)):\n",
    "            batch = [[e.to(device) for e in item] if isinstance(item, list) else item.to(device) for item in batch]\n",
    "            feature_ids, feature_vals, labels, support_data = batch\n",
    "            outputs, _, _, _ = model(feature_ids, feature_vals, support_data)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            pred_arr = np.hstack(\n",
    "                [pred_arr, outputs.data.detach().cpu()]) if pred_arr.size else outputs.data.detach().cpu()\n",
    "            label_arr = np.hstack(\n",
    "                [label_arr, labels.data.detach().cpu()]) if label_arr.size else labels.data.detach().cpu()\n",
    "        val_loss = running_loss / (itr + 1)\n",
    "        torch.cuda.empty_cache()\n",
    "    auc = roc_auc_score(label_arr, pred_arr)\n",
    "    return val_loss, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DeepFM base learner, which will be used in the feature encoder and the shared predictor, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFM_encoder(nn.Module):\n",
    "    def __init__(self, num_features, embedding_dim, num_fields, hidden_size=400):\n",
    "        super(DeepFM_encoder, self).__init__()\n",
    "#         num_fields -= 1\n",
    "        self.num_features = num_features\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_fields = num_fields\n",
    "        self.last_layer_dim = 400\n",
    "        self.feature_embeddings = nn.Embedding(num_features, embedding_dim)\n",
    "        torch.nn.init.xavier_normal_(self.feature_embeddings.weight)\n",
    "        self.input_dim = embedding_dim * num_fields\n",
    "        self.fc1 = nn.Linear(self.input_dim, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, self.last_layer_dim)\n",
    "        self.fc4 = nn.Linear(self.last_layer_dim+self.embedding_dim, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, feature_ids, feature_vals, return_hidden=False):\n",
    "#         # exclude uid feature field\n",
    "#         feature_ids = feature_ids[:,1:]\n",
    "#         feature_vals = feature_vals[:,1:]\n",
    "        # None*F*K\n",
    "        input_embeddings = self.feature_embeddings(feature_ids)\n",
    "        input_embeddings *= feature_vals.unsqueeze(dim=2)\n",
    "        # None*K\n",
    "        square_sum = torch.sum(input_embeddings ** 2, dim=1)\n",
    "        sum_square = torch.sum(input_embeddings, dim=1) ** 2\n",
    "        # None*K\n",
    "        hidden_fm = (sum_square - square_sum) / 2\n",
    "        # None*(F*K)\n",
    "        input_embeddings_flatten = input_embeddings.view(-1, self.input_dim)\n",
    "        hidden = nn.ReLU()(self.fc1(input_embeddings_flatten))\n",
    "        hidden = nn.ReLU()(self.fc2(hidden))\n",
    "        hidden_dnn =  nn.ReLU()(self.fc3(hidden))\n",
    "        hidden_encoder = torch.cat([hidden_fm, hidden_dnn],dim=1)\n",
    "        prediction = self.fc4(hidden_encoder).squeeze(1)\n",
    "        if return_hidden:\n",
    "            return prediction, hidden_encoder\n",
    "        else:\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the shared predictor $\\Psi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/690 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 15:54:19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 690/690 [00:08<00:00, 78.44it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 0: 0.6036580337994341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_df=pd.read_csv(PATH+'train_df.csv')\n",
    "val_df = pd.read_csv(PATH+f'valid_df.csv')\n",
    "test_df = pd.read_csv(PATH+f'test_df.csv')\n",
    "\n",
    "# dataframe->pytorch dataset\n",
    "train_dataset = CTR_Dataset(train_df)\n",
    "val_dataset = CTR_Dataset(val_df)\n",
    "test_dataset = CTR_Dataset(test_df)\n",
    "num_fields = train_dataset.num_fields\n",
    "num_features = 1+max([x.x_id.max().item() for x in [train_dataset, val_dataset, test_dataset]])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "# Define model and optimizer.\n",
    "model = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "# Start training.\n",
    "best_loss = np.inf\n",
    "best_epoch = -1\n",
    "best_auc = 0.5\n",
    "for epoch in range(1):\n",
    "    print(f\"Starting epoch: {epoch} | phase: train | ⏰: {time.strftime('%H:%M:%S')}\")\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for itr, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = [item.to(device) for item in batch]\n",
    "        feature_ids, feature_vals, labels = batch\n",
    "        if feature_ids.shape[0]==1:\n",
    "            break\n",
    "        outputs = model(feature_ids, feature_vals).squeeze()\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        running_loss += loss.detach().item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    epoch_loss = running_loss / (itr+1)\n",
    "    print(f\"training loss of epoch {epoch}: {epoch_loss}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    state = {\n",
    "    \"epoch_loss\": epoch_loss,\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(state, f\"predictor-{exp_id}.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test | ⏰: 15:59:04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188it [00:01, 172.56it/s]\n",
      "186it [00:01, 167.22it/s]\n",
      "185it [00:00, 237.31it/s]\n",
      "184it [00:00, 203.46it/s]\n",
      "183it [00:00, 203.25it/s]\n",
      "182it [00:01, 145.26it/s]\n",
      "180it [00:00, 190.57it/s]\n",
      "179it [00:01, 170.36it/s]\n",
      "178it [00:00, 192.21it/s]\n",
      "177it [00:00, 214.38it/s]\n",
      "176it [00:00, 231.89it/s]\n",
      "175it [00:00, 190.51it/s]\n",
      "173it [00:01, 159.46it/s]\n",
      "172it [00:01, 164.46it/s]\n",
      "171it [00:00, 192.83it/s]\n",
      "170it [00:00, 194.24it/s]\n",
      "169it [00:00, 200.67it/s]\n",
      "167it [00:00, 207.23it/s]\n",
      "166it [00:01, 141.60it/s]\n",
      "165it [00:00, 185.17it/s]\n",
      "164it [00:00, 167.30it/s]\n",
      "163it [00:00, 167.97it/s]\n",
      "162it [00:00, 236.65it/s]\n",
      "161it [00:00, 216.79it/s]\n",
      "159it [00:00, 199.41it/s]\n",
      "158it [00:01, 153.71it/s]\n",
      "157it [00:00, 180.42it/s]\n",
      "156it [00:01, 148.26it/s]\n",
      "155it [00:00, 162.54it/s]\n",
      "154it [00:00, 184.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cold start I: Loss: 0.6021976912701057, auc: 0.7213255798748595\n",
      "cold start II: Loss: 0.6048084336442361, auc: 0.7212235783528799\n",
      "cold start III: Loss: 0.6067371029609409, auc: 0.7212622311097989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fine-grained test on base model\n",
    "print(f\"Starting test | ⏰: {time.strftime('%H:%M:%S')}\")\n",
    "model = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "model = model.to(device)\n",
    "checkpoint = torch.load(f\"predictor-{exp_id}.tar\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "base_test_losses = []\n",
    "base_test_aucs = []\n",
    "\n",
    "for i in rqdm(range(1,COLD_USER_THRESHOLD+1,1)):\n",
    "    # omit users with <= i interactions.\n",
    "    test_support_set = test_df.groupby('uid',as_index=False).apply(\n",
    "        lambda x: x[:i] if len(x)>i else x[:0])\n",
    "    test_query_set = test_df.groupby('uid',as_index=False).apply(\n",
    "        lambda x: x[i:] if len(x)>i else x[:0])\n",
    "    test_dataset = CTR_Dataset(test_query_set)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    test_loss, test_auc = val(model, test_dataloader)\n",
    "#     print(f\"test loss of user group {i}: {test_loss}, auc: {test_auc}, gauc: {test_gauc}\")\n",
    "\n",
    "    base_test_losses += [test_loss]\n",
    "    base_test_aucs += [test_auc]\n",
    "\n",
    "print(f\"cold start I: Loss: {sum(base_test_losses[:10])/10}, auc: {sum(base_test_aucs[:10])/10}\")\n",
    "print(f\"cold start II: Loss: {sum(base_test_losses[10:20])/10}, auc: {sum(base_test_aucs[10:20])/10}\")\n",
    "print(f\"cold start III: Loss: {sum(base_test_losses[20:30])/10}, auc: {sum(base_test_aucs[20:30])/10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RESUS model with NN base learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustLayer(nn.Module):\n",
    "    def __init__(self, init_scale=0.4, num_adjust=None, init_bias=0, base=1):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([init_scale for i in range(num_adjust)]).unsqueeze(1))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor([init_bias for i in range(num_adjust)]).unsqueeze(1))\n",
    "\n",
    "    def forward(self, x, num_samples):\n",
    "        return x * (10**self.scale[num_samples-1]) + self.bias[num_samples-1]\n",
    "\n",
    "class RESUS_NN(nn.Module):\n",
    "    def __init__(self, num_fields, COLD_USER_THRESHOLD, encoder, predictor):\n",
    "        super(RESUS_NN, self).__init__()\n",
    "        self.num_fields = num_fields\n",
    "        self.COLD_USER_THRESHOLD = COLD_USER_THRESHOLD\n",
    "        self.predictor = predictor\n",
    "        self.encoder = encoder\n",
    "        self.L = nn.CrossEntropyLoss()\n",
    "        self.adjust = AdjustLayer(num_adjust=COLD_USER_THRESHOLD)\n",
    "        self.fc1 = nn.Linear(self.encoder.last_layer_dim+self.encoder.embedding_dim, 1)\n",
    "\n",
    "    def forward(self, feature_ids, feature_vals, support_data, debug=False):\n",
    "        # feature_ids: None*num_fields\n",
    "        # feature_vals: None*num_fields\n",
    "        # support_data: [x_id_support,x_val_support,y_support]\n",
    "        # x_id_support: None*COLD_USER_THRESHOLD*num_fields\n",
    "        # x_val_support: None*COLD_USER_THRESHOLD*num_fields\n",
    "        # y_support: None*COLD_USER_THRESHOLD\n",
    "        \n",
    "        x_id_support, x_val_support, y_support = support_data\n",
    "        feature_ids_concat = torch.cat([feature_ids.unsqueeze(1),x_id_support],dim=1) # None*(COLD_USER_THRESHOLD+1)*num_fields\n",
    "        feature_vals_concat = torch.cat([feature_vals.unsqueeze(1),x_val_support],dim=1) # None*(COLD_USER_THRESHOLD+1)*num_fields\n",
    "        feature_ids_concat = feature_ids_concat.view(-1,self.num_fields) # (None*(COLD_USER_THRESHOLD+1))*num_fields\n",
    "        feature_vals_concat = feature_vals_concat.view(-1,self.num_fields) # (None*(COLD_USER_THRESHOLD+1))*num_fields\n",
    "        output_predictor = self.predictor(feature_ids_concat, feature_vals_concat, return_hidden=False)\n",
    "        output_predictor = output_predictor.view(-1, self.COLD_USER_THRESHOLD+1) # None*(COLD_USER_THRESHOLD+1)\n",
    "        _, g_x_concat = self.encoder(feature_ids_concat, feature_vals_concat, return_hidden=True)\n",
    "        g_x_concat = g_x_concat.view(-1, self.COLD_USER_THRESHOLD+1, g_x_concat.shape[1]) # None*(COLD_USER_THRESHOLD+1)*hidden_size\n",
    "        g_x_hat = g_x_concat[:,[0],:] # None*1*hidden_size\n",
    "        g_x_support = g_x_concat[:,1:,:] # None*COLD_USER_THRESHOLD*hidden_size\n",
    "        num_samples = (y_support!=-1).sum(1) # None    \n",
    "        distance = torch.abs(g_x_hat-g_x_support) # None*COLD_USER_THRESHOLD*hidden_size\n",
    "        similar_score = self.fc1(distance).squeeze() # None*COLD_USER_THRESHOLD\n",
    "        support_mask = (y_support==-1) # None*COLD_USER_THRESHOLD\n",
    "        similar_score[support_mask] = float('-inf')\n",
    "        similar_score_normalized = nn.Softmax(dim=1)(similar_score*1) # None*COLD_USER_THRESHOLD \n",
    "        delta_y = y_support-nn.Sigmoid()(output_predictor[:,1:]) #None*COLD_USER_THRESHOLD\n",
    "        delta_y_hat = (delta_y*similar_score_normalized).sum(1,keepdim=True) # None\n",
    "        prediction = self.adjust(delta_y_hat, num_samples) + output_predictor[:,[0]]        \n",
    "        if debug:\n",
    "            return X_nomask, X, y_support, nn.Sigmoid()(matmul(X, W)), matmul(X, delta_W), delta_W\n",
    "        else:\n",
    "            return prediction.squeeze(), y_support-nn.Sigmoid()(output_predictor[:,1:]),similar_score_normalized, delta_y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RESUS_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:40<00:00,  2.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 16:31:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626/626 [04:33<00:00,  2.29it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 0: 0.5520654066492574\n",
      "Starting epoch: 0 | phase: val | ⏰: 16:35:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:38<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss of epoch 0: 0.5791232498680673, auc: 0.7604671031665173\n",
      "******** New optimal found, saving state ********\n",
      "Starting epoch: 1 | phase: train | ⏰: 16:36:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 627/627 [04:27<00:00,  2.35it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 1: 0.5502091677565324\n",
      "Starting epoch: 1 | phase: val | ⏰: 16:40:55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:36<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss of epoch 1: 0.5750143135466227, auc: 0.7656053336739735\n",
      "******** New optimal found, saving state ********\n",
      "Starting epoch: 2 | phase: train | ⏰: 16:41:31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 627/627 [04:31<00:00,  2.31it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 2: 0.5479272462344436\n",
      "Starting epoch: 2 | phase: val | ⏰: 16:46:07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:37<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss of epoch 2: 0.5736086535744551, auc: 0.7669373538071427\n",
      "******** New optimal found, saving state ********\n",
      "Starting epoch: 3 | phase: train | ⏰: 16:46:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 73/625 [00:33<04:16,  2.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-8563e63b1f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mresus_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_query_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfeature_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load encoder\n",
    "model = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "model = model.to(device)\n",
    "checkpoint = torch.load(f\"predictor-{exp_id}.tar\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "encoder = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "resus_nn = RESUS_NN(num_fields, COLD_USER_THRESHOLD, encoder, model).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\": resus_nn.encoder.parameters(), \"lr\": 0.001},\n",
    "        {\"params\": resus_nn.fc1.parameters(), \"lr\": 0.001},\n",
    "        {\"params\": resus_nn.adjust.parameters(), \"lr\": 0.001},\n",
    "    ],\n",
    ")\n",
    "\n",
    "best_loss = np.inf\n",
    "best_epoch = -1\n",
    "best_auc = 0.5\n",
    "train_df_gb_uid = train_df.groupby('uid')\n",
    "num_users = max(train_df_gb_uid.groups.keys())+1\n",
    "\n",
    "val_df_support = pd.read_csv(f'../data/val_df_support.csv')\n",
    "val_df_query = pd.read_csv(f'../data/val_df_query.csv')\n",
    "\n",
    "val_query_dataset = QueryWithSupportDataset(val_df_query,val_df_support,COLD_USER_THRESHOLD)\n",
    "val_query_dataloader = DataLoader(val_query_dataset, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch: {epoch} | phase: train | ⏰: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "    def sample_func(x):\n",
    "        num_sample = np.random.randint(1,COLD_USER_THRESHOLD+1)\n",
    "        if len(x)>num_sample:\n",
    "            return x.sample(n=num_sample)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    train_support_df = train_df_gb_uid.apply(sample_func).reset_index(level=0, drop=True)\n",
    "    train_query_df = pd.concat([train_df, train_support_df]).drop_duplicates(keep=False)\n",
    "    train_query_dataset = QueryWithSupportDataset(train_query_df,train_support_df,COLD_USER_THRESHOLD)\n",
    "    train_query_dataloader = DataLoader(train_query_dataset, batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # Start training\n",
    "    resus_nn.train()\n",
    "    running_loss = 0\n",
    "    for itr, batch in enumerate(tqdm(train_query_dataloader)):\n",
    "        batch = [[e.to(device) for e in item] if isinstance(item, list) else item.to(device) for item in batch]\n",
    "        feature_ids, feature_vals, labels, support_data = batch\n",
    "        outputs, predictor, delta_W_X, delta_W = resus_nn(feature_ids, feature_vals, support_data)            \n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        running_loss += loss.detach().item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    epoch_loss = running_loss / (itr+1)\n",
    "    print(f\"training loss of epoch {epoch}: {epoch_loss}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Starting epoch: {epoch} | phase: val | ⏰: {time.strftime('%H:%M:%S')}\")\n",
    "    state = {\n",
    "    \"epoch\": epoch,\n",
    "    \"best_loss\": best_loss,\n",
    "    \"best_auc\": best_auc,\n",
    "    \"model\": resus_nn.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    resus_nn.eval()\n",
    "    val_loss, val_auc = val_query(resus_nn, val_query_dataloader)\n",
    "    print(f\"validation loss of epoch {epoch}: {val_loss}, auc: {val_auc}\")\n",
    "    if val_auc > best_auc:\n",
    "        print(\"******** New optimal found, saving state ********\")\n",
    "        patience = overfit_patience\n",
    "        state[\"best_loss\"] = best_loss = val_loss\n",
    "        state[\"best_auc\"] = best_auc = val_auc\n",
    "        best_epoch = epoch\n",
    "        torch.save(state, f\"RESUS_NN-{exp_id}.tar\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "    if optimizer.param_groups[0]['lr'] <= 1e-7:\n",
    "        print('LR less than 1e-7, stop training...')\n",
    "        break\n",
    "    if patience == 0:\n",
    "        print('patience == 0, stop training...')\n",
    "        break\n",
    "    del train_support_df\n",
    "    del train_query_df\n",
    "    del train_query_dataset\n",
    "    del train_query_dataloader\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test | ⏰: 20:01:26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/178 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/178 [00:03<11:28,  3.89s/it]\u001b[A\n",
      "  1%|          | 2/178 [00:04<05:15,  1.79s/it]\u001b[A\n",
      "  3%|▎         | 6/178 [00:04<01:34,  1.83it/s]\u001b[A\n",
      "  5%|▌         | 9/178 [00:05<01:05,  2.57it/s]\u001b[A\n",
      "  6%|▌         | 10/178 [00:05<01:02,  2.70it/s]\u001b[A\n",
      "  8%|▊         | 14/178 [00:06<00:46,  3.54it/s]\u001b[A\n",
      " 10%|▉         | 17/178 [00:07<00:39,  4.05it/s]\u001b[A\n",
      " 10%|█         | 18/178 [00:07<00:42,  3.79it/s]\u001b[A\n",
      " 12%|█▏        | 22/178 [00:08<00:35,  4.43it/s]\u001b[A\n",
      " 14%|█▍        | 25/178 [00:08<00:33,  4.60it/s]\u001b[A\n",
      " 15%|█▍        | 26/178 [00:09<00:34,  4.43it/s]\u001b[A\n",
      " 17%|█▋        | 30/178 [00:09<00:31,  4.70it/s]\u001b[A\n",
      " 19%|█▊        | 33/178 [00:10<00:29,  4.85it/s]\u001b[A\n",
      " 19%|█▉        | 34/178 [00:10<00:29,  4.81it/s]\u001b[A\n",
      " 21%|██        | 37/178 [00:10<00:22,  6.18it/s]\u001b[A\n",
      " 21%|██▏       | 38/178 [00:11<00:33,  4.19it/s]\u001b[A\n",
      " 23%|██▎       | 41/178 [00:12<00:29,  4.59it/s]\u001b[A\n",
      " 24%|██▎       | 42/178 [00:12<00:27,  4.95it/s]\u001b[A\n",
      " 24%|██▍       | 43/178 [00:12<00:26,  5.03it/s]\u001b[A\n",
      " 25%|██▌       | 45/178 [00:12<00:22,  5.88it/s]\u001b[A\n",
      " 26%|██▌       | 46/178 [00:13<00:34,  3.87it/s]\u001b[A\n",
      " 28%|██▊       | 49/178 [00:13<00:28,  4.58it/s]\u001b[A\n",
      " 29%|██▊       | 51/178 [00:14<00:27,  4.60it/s]\u001b[A\n",
      " 30%|██▉       | 53/178 [00:14<00:21,  5.71it/s]\u001b[A\n",
      " 30%|███       | 54/178 [00:14<00:30,  4.04it/s]\u001b[A\n",
      " 32%|███▏      | 57/178 [00:15<00:24,  4.92it/s]\u001b[A\n",
      " 33%|███▎      | 58/178 [00:15<00:23,  5.22it/s]\u001b[A\n",
      " 33%|███▎      | 59/178 [00:15<00:27,  4.26it/s]\u001b[A\n",
      " 34%|███▍      | 61/178 [00:16<00:21,  5.50it/s]\u001b[A\n",
      " 35%|███▍      | 62/178 [00:16<00:30,  3.85it/s]\u001b[A\n",
      " 37%|███▋      | 65/178 [00:17<00:21,  5.15it/s]\u001b[A\n",
      " 37%|███▋      | 66/178 [00:17<00:20,  5.57it/s]\u001b[A\n",
      " 38%|███▊      | 67/178 [00:17<00:26,  4.24it/s]\u001b[A\n",
      " 39%|███▉      | 69/178 [00:17<00:21,  5.03it/s]\u001b[A\n",
      " 39%|███▉      | 70/178 [00:18<00:26,  4.12it/s]\u001b[A\n",
      " 41%|████      | 73/178 [00:18<00:19,  5.33it/s]\u001b[A\n",
      " 42%|████▏     | 75/178 [00:19<00:23,  4.48it/s]\u001b[A\n",
      " 43%|████▎     | 77/178 [00:19<00:22,  4.46it/s]\u001b[A\n",
      " 44%|████▍     | 78/178 [00:19<00:22,  4.49it/s]\u001b[A\n",
      " 46%|████▌     | 81/178 [00:20<00:18,  5.18it/s]\u001b[A\n",
      " 47%|████▋     | 83/178 [00:21<00:21,  4.42it/s]\u001b[A\n",
      " 48%|████▊     | 85/178 [00:21<00:22,  4.12it/s]\u001b[A\n",
      " 50%|█████     | 89/178 [00:22<00:16,  5.34it/s]\u001b[A\n",
      " 51%|█████     | 91/178 [00:22<00:18,  4.71it/s]\u001b[A\n",
      " 52%|█████▏    | 93/178 [00:23<00:21,  3.89it/s]\u001b[A\n",
      " 54%|█████▍    | 97/178 [00:23<00:15,  5.20it/s]\u001b[A\n",
      " 56%|█████▌    | 99/178 [00:24<00:16,  4.93it/s]\u001b[A\n",
      " 57%|█████▋    | 101/178 [00:25<00:20,  3.83it/s]\u001b[A\n",
      " 59%|█████▉    | 105/178 [00:25<00:13,  5.42it/s]\u001b[A\n",
      " 60%|██████    | 107/178 [00:25<00:14,  5.07it/s]\u001b[A\n",
      " 61%|██████    | 109/178 [00:26<00:17,  3.84it/s]\u001b[A\n",
      " 63%|██████▎   | 113/178 [00:27<00:11,  5.64it/s]\u001b[A\n",
      " 65%|██████▍   | 115/178 [00:27<00:12,  5.15it/s]\u001b[A\n",
      " 65%|██████▌   | 116/178 [00:27<00:12,  5.16it/s]\u001b[A\n",
      " 66%|██████▌   | 117/178 [00:28<00:17,  3.54it/s]\u001b[A\n",
      " 68%|██████▊   | 121/178 [00:28<00:09,  5.71it/s]\u001b[A\n",
      " 69%|██████▉   | 123/178 [00:29<00:10,  5.19it/s]\u001b[A\n",
      " 70%|██████▉   | 124/178 [00:29<00:10,  4.98it/s]\u001b[A\n",
      " 70%|███████   | 125/178 [00:30<00:14,  3.56it/s]\u001b[A\n",
      " 72%|███████▏  | 129/178 [00:30<00:08,  5.84it/s]\u001b[A\n",
      " 74%|███████▎  | 131/178 [00:30<00:09,  5.20it/s]\u001b[A\n",
      " 74%|███████▍  | 132/178 [00:31<00:09,  4.69it/s]\u001b[A\n",
      " 75%|███████▍  | 133/178 [00:31<00:12,  3.59it/s]\u001b[A\n",
      " 77%|███████▋  | 137/178 [00:32<00:06,  6.15it/s]\u001b[A\n",
      " 78%|███████▊  | 139/178 [00:32<00:07,  5.30it/s]\u001b[A\n",
      " 79%|███████▊  | 140/178 [00:32<00:08,  4.60it/s]\u001b[A\n",
      " 79%|███████▉  | 141/178 [00:33<00:10,  3.52it/s]\u001b[A\n",
      " 81%|████████▏ | 145/178 [00:33<00:05,  6.27it/s]\u001b[A\n",
      " 83%|████████▎ | 147/178 [00:34<00:05,  5.30it/s]\u001b[A\n",
      " 83%|████████▎ | 148/178 [00:34<00:06,  4.48it/s]\u001b[A\n",
      " 84%|████████▎ | 149/178 [00:35<00:08,  3.54it/s]\u001b[A\n",
      " 86%|████████▌ | 153/178 [00:35<00:04,  5.74it/s]\u001b[A\n",
      " 87%|████████▋ | 155/178 [00:35<00:04,  5.20it/s]\u001b[A\n",
      " 88%|████████▊ | 156/178 [00:36<00:05,  4.28it/s]\u001b[A\n",
      " 88%|████████▊ | 157/178 [00:36<00:06,  3.46it/s]\u001b[A\n",
      " 90%|█████████ | 161/178 [00:37<00:02,  5.99it/s]\u001b[A\n",
      " 92%|█████████▏| 163/178 [00:37<00:02,  5.33it/s]\u001b[A\n",
      " 92%|█████████▏| 164/178 [00:37<00:03,  4.48it/s]\u001b[A\n",
      " 93%|█████████▎| 165/178 [00:38<00:03,  3.57it/s]\u001b[A\n",
      " 95%|█████████▍| 169/178 [00:38<00:01,  6.19it/s]\u001b[A\n",
      " 96%|█████████▌| 171/178 [00:39<00:01,  5.90it/s]\u001b[A\n",
      " 97%|█████████▋| 172/178 [00:39<00:01,  4.62it/s]\u001b[A\n",
      " 97%|█████████▋| 173/178 [00:40<00:01,  3.79it/s]\u001b[A\n",
      "100%|██████████| 178/178 [00:40<00:00,  4.39it/s]\u001b[A\n",
      "  3%|▎         | 1/30 [00:41<19:52, 41.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss of user group 1: 0.5944164870830064, auc: 0.7232352468956003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/176 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/176 [00:02<07:19,  2.51s/it]\u001b[A\n",
      "  1%|          | 2/176 [00:02<03:29,  1.20s/it]\u001b[A\n",
      "  5%|▍         | 8/176 [00:02<00:35,  4.68it/s]\u001b[A\n",
      "  6%|▋         | 11/176 [00:04<00:52,  3.16it/s]\u001b[A\n",
      " 10%|▉         | 17/176 [00:05<00:41,  3.79it/s]\u001b[A\n",
      " 11%|█         | 19/176 [00:06<00:38,  4.04it/s]\u001b[A\n",
      " 14%|█▍        | 25/176 [00:07<00:34,  4.34it/s]\u001b[A\n",
      " 15%|█▍        | 26/176 [00:07<00:36,  4.06it/s]\u001b[A\n",
      " 18%|█▊        | 32/176 [00:07<00:20,  7.05it/s]\u001b[A\n",
      " 20%|█▉        | 35/176 [00:09<00:32,  4.34it/s]\u001b[A\n",
      " 23%|██▎       | 40/176 [00:09<00:22,  6.07it/s]\u001b[A\n",
      " 24%|██▍       | 42/176 [00:11<00:35,  3.76it/s]\u001b[A\n",
      " 26%|██▌       | 46/176 [00:11<00:25,  5.19it/s]\u001b[A\n",
      " 27%|██▋       | 48/176 [00:11<00:22,  5.80it/s]\u001b[A\n",
      " 28%|██▊       | 50/176 [00:12<00:36,  3.42it/s]\u001b[A\n",
      " 31%|███       | 54/176 [00:13<00:24,  5.07it/s]\u001b[A\n",
      " 32%|███▏      | 56/176 [00:13<00:20,  5.98it/s]\u001b[A\n",
      " 33%|███▎      | 58/176 [00:14<00:34,  3.43it/s]\u001b[A\n",
      " 34%|███▍      | 60/176 [00:14<00:27,  4.25it/s]\u001b[A\n",
      " 35%|███▌      | 62/176 [00:14<00:21,  5.35it/s]\u001b[A\n",
      " 37%|███▋      | 65/176 [00:15<00:22,  5.03it/s]\u001b[A\n",
      " 38%|███▊      | 67/176 [00:16<00:28,  3.77it/s]\u001b[A\n",
      " 40%|███▉      | 70/176 [00:16<00:19,  5.38it/s]\u001b[A\n",
      " 41%|████▏     | 73/176 [00:17<00:19,  5.19it/s]\u001b[A\n",
      " 43%|████▎     | 75/176 [00:18<00:28,  3.60it/s]\u001b[A\n",
      " 45%|████▌     | 80/176 [00:18<00:15,  6.28it/s]\u001b[A\n",
      " 47%|████▋     | 82/176 [00:19<00:26,  3.55it/s]\u001b[A\n",
      " 48%|████▊     | 84/176 [00:20<00:22,  4.10it/s]\u001b[A\n",
      " 51%|█████     | 89/176 [00:20<00:15,  5.46it/s]\u001b[A\n",
      " 52%|█████▏    | 91/176 [00:21<00:24,  3.53it/s]\u001b[A\n",
      " 53%|█████▎    | 94/176 [00:22<00:20,  4.04it/s]\u001b[A\n",
      " 56%|█████▌    | 98/176 [00:23<00:18,  4.22it/s]\u001b[A\n",
      " 56%|█████▋    | 99/176 [00:23<00:19,  3.89it/s]\u001b[A\n",
      " 60%|█████▉    | 105/176 [00:23<00:10,  6.91it/s]\u001b[A\n",
      " 61%|██████    | 107/176 [00:25<00:19,  3.62it/s]\u001b[A\n",
      " 65%|██████▍   | 114/176 [00:26<00:13,  4.62it/s]\u001b[A\n",
      " 65%|██████▌   | 115/176 [00:27<00:15,  3.83it/s]\u001b[A\n",
      " 68%|██████▊   | 119/176 [00:27<00:10,  5.54it/s]\u001b[A\n",
      " 69%|██████▉   | 122/176 [00:28<00:11,  4.85it/s]\u001b[A\n",
      " 70%|███████   | 124/176 [00:29<00:13,  3.93it/s]\u001b[A\n",
      " 74%|███████▍  | 130/176 [00:29<00:08,  5.12it/s]\u001b[A\n",
      " 74%|███████▍  | 131/176 [00:30<00:11,  3.76it/s]\u001b[A\n",
      " 78%|███████▊  | 138/176 [00:31<00:07,  5.39it/s]\u001b[A\n",
      " 79%|███████▉  | 139/176 [00:32<00:09,  3.97it/s]\u001b[A\n",
      " 82%|████████▏ | 144/176 [00:32<00:05,  6.23it/s]\u001b[A\n",
      " 83%|████████▎ | 146/176 [00:33<00:05,  5.26it/s]\u001b[A\n",
      " 84%|████████▍ | 148/176 [00:34<00:06,  4.05it/s]\u001b[A\n",
      " 88%|████████▊ | 154/176 [00:34<00:04,  5.15it/s]\u001b[A\n",
      " 88%|████████▊ | 155/176 [00:35<00:05,  3.72it/s]\u001b[A\n",
      " 92%|█████████▏| 162/176 [00:36<00:02,  5.40it/s]\u001b[A\n",
      " 93%|█████████▎| 163/176 [00:37<00:03,  3.94it/s]\u001b[A\n",
      " 97%|█████████▋| 170/176 [00:38<00:01,  5.56it/s]\u001b[A\n",
      "100%|██████████| 176/176 [00:39<00:00,  4.48it/s]\u001b[A\n",
      "  7%|▋         | 2/30 [01:20<18:46, 40.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss of user group 2: 0.5991324579173868, auc: 0.7247234581584001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/175 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 1/175 [00:05<14:39,  5.05s/it]\u001b[A\n",
      "  5%|▍         | 8/175 [00:06<02:16,  1.22it/s]\u001b[A\n",
      "  7%|▋         | 2/30 [01:27<20:27, 43.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-027a4b4d5a54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtest_query_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueryWithSupportDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_query_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_support_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOLD_USER_THRESHOLD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtest_query_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_query_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresus_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_query_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"test loss of user group {i}: {test_loss}, auc: {test_auc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-9c3441ae2d25>\u001b[0m in \u001b[0;36mval_query\u001b[0;34m(model, val_dataloader)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mlabel_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mfeature_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fine-grained test on nn model\n",
    "print(f\"Starting test | ⏰: {time.strftime('%H:%M:%S')}\")\n",
    "model = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "encoder = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "resus_nn = RESUS_NN(num_fields, COLD_USER_THRESHOLD, encoder, model).to(device)\n",
    "checkpoint = torch.load(f\"RESUS_NN-{exp_id}.tar\", map_location=torch.device('cpu'))\n",
    "resus_nn.load_state_dict(checkpoint['model'])\n",
    "\n",
    "resus_nn_test_losses = []\n",
    "resus_nn_test_aucs = []\n",
    "\n",
    "for i in tqdm(range(1,COLD_USER_THRESHOLD+1,1)):\n",
    "    test_support_set = pd.read_csv(f'../data/test/test_df_support_{i}.csv')\n",
    "    test_query_set = pd.read_csv(f'../data/test/test_df_query_{i}.csv')\n",
    "    test_query_dataset = QueryWithSupportDataset(test_query_set,test_support_set, COLD_USER_THRESHOLD)\n",
    "    test_query_dataloader = DataLoader(test_query_dataset, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    test_loss, test_auc = val_query(resus_nn, test_query_dataloader)\n",
    "    \n",
    "    print(f\"test loss of user group {i}: {test_loss}, auc: {test_auc}\")\n",
    "    resus_nn_test_losses += [test_loss]\n",
    "    resus_nn_test_aucs += [test_auc]\n",
    "    \n",
    "    del test_support_set\n",
    "    del test_query_set\n",
    "    del test_query_dataset\n",
    "    del test_query_dataloader\n",
    "\n",
    "print(f\"cold start I: Loss: {sum(resus_nn_test_losses[:10])/10}, auc: {sum(resus_nn_test_aucs[:10])/10}\")\n",
    "print(f\"cold start II: Loss: {sum(resus_nn_test_losses[10:20])/10}, auc: {sum(resus_nn_test_aucs[10:20])/10}\")\n",
    "print(f\"cold start III: Loss: {sum(resus_nn_test_losses[20:30])/10}, auc: {sum(resus_nn_test_aucs[20:30])/10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AdjustLayer(nn.Module):\n",
    "#     def __init__(self, init_scale=1, num_adjust=None, init_bias=0, base=1):\n",
    "#         super().__init__()\n",
    "#         self.scale = nn.Parameter(torch.FloatTensor([init_scale]))\n",
    "#         self.bias = nn.Parameter(torch.FloatTensor([init_bias]))\n",
    "#         self.base = base\n",
    "\n",
    "#     def forward(self, x, num_samples):\n",
    "#         return x * self.scale + self.bias\n",
    "\n",
    "class AdjustLayer(nn.Module):\n",
    "    def __init__(self, init_scale=0.4, num_adjust=None, init_bias=0, base=1):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([init_scale for i in range(num_adjust)]).unsqueeze(1))\n",
    "        self.bias = nn.Parameter(torch.FloatTensor([init_bias for i in range(num_adjust)]).unsqueeze(1))\n",
    "\n",
    "    def forward(self, x, num_samples):\n",
    "        return x * (10**self.scale[num_samples-1]) + self.bias[num_samples-1]\n",
    "\n",
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, learn_lambda=True, num_lambda=None, init_lambda=0.001, base=1):\n",
    "        super().__init__()\n",
    "        self.l = torch.FloatTensor([init_lambda]) # COLD\n",
    "        self.base = base\n",
    "        self.l = nn.Parameter(self.l, requires_grad=learn_lambda)\n",
    "\n",
    "    def forward(self, x, n_samples):\n",
    "        #   x: None*COLD*COLD\n",
    "        #   n_samples: None\n",
    "        return x * torch.abs(self.l.unsqueeze(1).unsqueeze(2))\n",
    "\n",
    "# RR\n",
    "class RESUS_RR(nn.Module):\n",
    "    def __init__(self, num_fields, COLD_USER_THRESHOLD, encoder, predictor):\n",
    "        super(RESUS_RR, self).__init__()\n",
    "        self.num_fields = num_fields\n",
    "        self.COLD_USER_THRESHOLD = COLD_USER_THRESHOLD\n",
    "        self.predictor = predictor\n",
    "        self.encoder = encoder\n",
    "        self.lambda_rr = LambdaLayer(learn_lambda=True, num_lambda=COLD_USER_THRESHOLD)\n",
    "        self.L = nn.CrossEntropyLoss()\n",
    "        self.adjust = AdjustLayer(1, num_adjust=COLD_USER_THRESHOLD)     \n",
    "        \n",
    "    def rr_standard(self, x, n_samples, yrr_binary, linsys=False):\n",
    "#         x /= n_samples\n",
    "        I = torch.eye(x.shape[1]).to(x)\n",
    "\n",
    "        if not linsys:\n",
    "            w = mm(mm(inv(mm(t(x, 0, 1), x) + self.lambda_rr(I)), t(x, 0, 1)), yrr_binary)\n",
    "        else:\n",
    "            A = mm(t_(x), x) + self.lambda_rr(I)\n",
    "            v = mm(t_(x), yrr_binary)\n",
    "            w, _ = solve(v, A)\n",
    "\n",
    "        return w\n",
    "\n",
    "    def rr_woodbury(self, X, n_samples, yrr_binary, linsys=False):\n",
    "        #   X: None*COLD_USER_THRESHOLD*(hidden_size+1)\n",
    "        #   n_samples: None\n",
    "#         x = X/torch.sqrt(n_samples.float()).unsqueeze(1).unsqueeze(2)    #   x: None*COLD*(hidden+1)\n",
    "        x = X\n",
    "        I = torch.eye(x.shape[1]).unsqueeze(0).repeat(x.shape[0],1,1).to(x)    # None*COLD*COLD\n",
    "        if not linsys:\n",
    "            w = matmul(matmul(t(x, 1, 2), inv(matmul(x, t(x, 1, 2)) + self.lambda_rr(I, n_samples))), yrr_binary)\n",
    "        else:\n",
    "            A = mm(x, t_(x)) + self.lambda_rr(I)\n",
    "            v = yrr_binary\n",
    "            w_, _ = solve(v, A)\n",
    "            w = mm(t_(x), w_)\n",
    "        return w\n",
    "\n",
    "    def forward(self, feature_ids, feature_vals, support_data, debug=False):\n",
    "        # feature_ids: None*num_fields\n",
    "        # feature_vals: None*num_fields\n",
    "        # support_data: [x_id_support,x_val_support,y_support]\n",
    "        # x_id_support: None*COLD_USER_THRESHOLD*num_fields\n",
    "        # x_val_support: None*COLD_USER_THRESHOLD*num_fields\n",
    "        # y_support: None*COLD_USER_THRESHOLD\n",
    "        \n",
    "        x_id_support, x_val_support, y_support = support_data\n",
    "        feature_ids_concat = torch.cat([feature_ids.unsqueeze(1),x_id_support],dim=1) # None*(COLD_USER_THRESHOLD+1)*num_fields\n",
    "        feature_vals_concat = torch.cat([feature_vals.unsqueeze(1),x_val_support],dim=1) # None*(COLD_USER_THRESHOLD+1)*num_fields\n",
    "        feature_ids_concat = feature_ids_concat.view(-1,self.num_fields) # (None*(COLD_USER_THRESHOLD+1))*num_fields\n",
    "        feature_vals_concat = feature_vals_concat.view(-1,self.num_fields) # (None*(COLD_USER_THRESHOLD+1))*num_fields\n",
    "        output_predictor = self.predictor(feature_ids_concat, feature_vals_concat, return_hidden=False)\n",
    "        output_predictor = output_predictor.view(-1, self.COLD_USER_THRESHOLD+1) # None*(COLD_USER_THRESHOLD+1)\n",
    "        _, g_x_concat = self.encoder(feature_ids_concat, feature_vals_concat, return_hidden=True)\n",
    "        g_x_concat = g_x_concat.view(-1, self.COLD_USER_THRESHOLD+1, g_x_concat.shape[1]) # None*(COLD_USER_THRESHOLD+1)*hidden_size\n",
    "        g_x_hat = g_x_concat[:,[0],:] # None*1*hidden_size\n",
    "        g_x_support = g_x_concat[:,1:,:] # None*COLD_USER_THRESHOLD*hidden_size\n",
    "        # output_encoder\n",
    "        y_x_hat = output_predictor[:,0] # None\n",
    "        X_mask = (y_support!=-1).int().float().unsqueeze(2) # None*COLD_USER_THRESHOLD*1\n",
    "        num_samples = (y_support!=-1).sum(1) # None\n",
    "        ones = torch.ones((g_x_support.shape[0],g_x_support.shape[1])).unsqueeze(2).to(g_x_hat) # None*COLD_USER_THRESHOLD*1\n",
    "        X_nomask = torch.cat((g_x_support, ones), 2) # None*COLD_USER_THRESHOLD*(hidden_size+1)\n",
    "        X = X_nomask*X_mask\n",
    "#         W = torch.cat([self.encoder.fc4.weight.squeeze(), self.encoder.fc4.bias]).unsqueeze(1) # (hidden_size+1)*1  \n",
    "        delta_W = self.rr_woodbury(X, num_samples, y_support.unsqueeze(2)-nn.Sigmoid()(output_predictor[:,1:].unsqueeze(2))) # None*(hidden_size+1)*1    \n",
    "        delta_w = delta_W[:,:-1] # None*(hidden_size)*1     \n",
    "        delta_b = delta_W[:,-1] # None*1     \n",
    "        out = matmul(g_x_hat, delta_w).squeeze(2) + delta_b # None*1\n",
    "        prediction = self.adjust(out, num_samples) + output_predictor[:,[0]]\n",
    "        if debug:\n",
    "            return X_nomask, X, y_support, nn.Sigmoid()(matmul(X, W)), matmul(X, delta_W), delta_W\n",
    "        else:\n",
    "            return prediction.squeeze(), y_support.unsqueeze(2)-nn.Sigmoid()(output_predictor[:,1:].unsqueeze(2)), matmul(X, delta_W), delta_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch: 0 | phase: train | ⏰: 22:00:25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 627/627 [04:31<00:00,  2.31it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 0: 0.6020512128370611\n",
      "Starting epoch: 0 | phase: val | ⏰: 22:05:02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:35<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "validation loss of epoch 0: 0.5822683681802052, auc: 0.7578080768274674, gauc: 0.7342\n",
      "******** New optimal found, saving state ********\n",
      "Starting epoch: 1 | phase: train | ⏰: 22:05:39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626/626 [04:22<00:00,  2.38it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 1: 0.5614815890408171\n",
      "Starting epoch: 1 | phase: val | ⏰: 22:10:08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:36<00:00,  2.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "validation loss of epoch 1: 0.5773067957744366, auc: 0.7612202199841563, gauc: 0.7342\n",
      "******** New optimal found, saving state ********\n",
      "Starting epoch: 2 | phase: train | ⏰: 22:10:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626/626 [04:44<00:00,  2.20it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 2: 0.5590110072693505\n",
      "Starting epoch: 2 | phase: val | ⏰: 22:15:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:43<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "validation loss of epoch 2: 0.5744686297527174, auc: 0.7646792612175957, gauc: 0.7376\n",
      "******** New optimal found, saving state ********\n",
      "Starting epoch: 3 | phase: train | ⏰: 22:16:21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626/626 [05:17<00:00,  1.97it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 3: 0.552932007529865\n",
      "Starting epoch: 3 | phase: val | ⏰: 22:21:45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:36<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "validation loss of epoch 3: 0.5741228403114691, auc: 0.7657776678512074, gauc: 0.7375\n",
      "******** New optimal found, saving state ********\n",
      "Starting epoch: 4 | phase: train | ⏰: 22:22:22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 627/627 [04:23<00:00,  2.38it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 4: 0.5475271836137087\n",
      "Starting epoch: 4 | phase: val | ⏰: 22:26:52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:36<00:00,  2.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "validation loss of epoch 4: 0.5687021751229356, auc: 0.771742974267069, gauc: 0.7404\n",
      "******** New optimal found, saving state ********\n",
      "Starting epoch: 5 | phase: train | ⏰: 22:27:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 626/626 [05:21<00:00,  1.95it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 5: 0.5443475951973241\n",
      "Starting epoch: 5 | phase: val | ⏰: 22:32:57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:39<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "validation loss of epoch 5: 0.5697358541372346, auc: 0.771053800178204, gauc: 0.7405\n",
      "Starting epoch: 6 | phase: train | ⏰: 22:33:38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 627/627 [04:32<00:00,  2.30it/s]\n",
      "  0%|          | 0/82 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss of epoch 6: 0.5428969441798696\n",
      "Starting epoch: 6 | phase: val | ⏰: 22:38:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [00:41<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "validation loss of epoch 6: 0.5720918353010969, auc: 0.7685317605616563, gauc: 0.7393\n",
      "patience == 0, stop training...\n"
     ]
    }
   ],
   "source": [
    "# load encoder\n",
    "model = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "model = model.to(device)\n",
    "checkpoint = torch.load(f\"checkpoint/predictor-{dataset}-{exp_id}.tar\", map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "encoder = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "resus_rr = RESUS_RR(num_fields, COLD_USER_THRESHOLD, encoder, model).to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {\"params\": resus_rr.encoder.parameters(), \"lr\": 0.001},\n",
    "        {\"params\": resus_rr.adjust.parameters(), \"lr\": 0.001},\n",
    "        {\"params\": resus_rr.lambda_rr.parameters(), \"lr\": 0.1},\n",
    "    ],\n",
    ")\n",
    "\n",
    "best_loss = np.inf\n",
    "best_epoch = -1\n",
    "best_auc = 0.5\n",
    "train_df_gb_uid = train_df.groupby('uid')\n",
    "num_users = max(train_df_gb_uid.groups.keys())+1\n",
    "\n",
    "val_df_support = val_df.groupby('uid').apply(lambda x: x[:COLD_USER_THRESHOLD] if len(x)>COLD_USER_THRESHOLD else x[:-1])\n",
    "val_df_query = val_df.groupby('uid').apply(lambda x: x[COLD_USER_THRESHOLD:] if len(x)>COLD_USER_THRESHOLD else x[-1:])\n",
    "\n",
    "val_query_dataset = QueryWithSupportDataset(val_df_query,val_df_support,COLD_USER_THRESHOLD)\n",
    "val_query_dataloader = DataLoader(val_query_dataset, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch: {epoch} | phase: train | ⏰: {time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "    # Random sample support set\n",
    "    def sample_func(x):\n",
    "        num_sample = np.random.randint(1,COLD_USER_THRESHOLD+1)\n",
    "        if len(x)>num_sample:\n",
    "            return x.sample(n=num_sample)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    train_support_df = train_df_gb_uid.apply(sample_func).reset_index(level=0, drop=True)\n",
    "    train_query_df = pd.concat([train_df, train_support_df]).drop_duplicates(keep=False)\n",
    "    train_query_dataset = QueryWithSupportDataset(train_query_df,train_support_df,COLD_USER_THRESHOLD)\n",
    "    train_query_dataloader = DataLoader(train_query_dataset, batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # Start training\n",
    "    rr.train()\n",
    "    running_loss = 0\n",
    "    for itr, batch in enumerate(tqdm(train_query_dataloader)):\n",
    "        batch = [[e.to(device) for e in item] if isinstance(item, list) else item.to(device) for item in batch]\n",
    "        feature_ids, feature_vals, labels, support_data = batch\n",
    "        outputs, delta_y_support, delta_W_X, delta_W = resus_rr(feature_ids, feature_vals, support_data)            \n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.detach().item()\n",
    "    epoch_loss = running_loss / (itr+1)\n",
    "    print(f\"training loss of epoch {epoch}: {epoch_loss}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Starting epoch: {epoch} | phase: val | ⏰: {time.strftime('%H:%M:%S')}\")\n",
    "    state = {\n",
    "    \"epoch\": epoch,\n",
    "    \"best_loss\": best_loss,\n",
    "    \"best_auc\": best_auc,\n",
    "    \"model\": resus_rr.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    resus_rr.eval()\n",
    "    val_loss, val_auc = val_query(resus_rr, val_query_dataloader, gauc_col=0)\n",
    "    print(f\"validation loss of epoch {epoch}: {val_loss}, auc: {val_auc}\")\n",
    "\n",
    "    if val_auc > best_auc:\n",
    "        print(\"******** New optimal found, saving state ********\")\n",
    "        patience = overfit_patience\n",
    "        state[\"best_loss\"] = best_loss = val_loss\n",
    "        state[\"best_auc\"] = best_auc = val_auc\n",
    "        best_epoch = epoch\n",
    "        torch.save(state, f\"RESUS_RR-{exp_id}.tar\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "    if optimizer.param_groups[0]['lr'] <= 1e-7:\n",
    "        print('LR less than 1e-7, stop training...')\n",
    "        break\n",
    "    if patience == 0:\n",
    "        print('patience == 0, stop training...')\n",
    "        break\n",
    "    del train_support_df\n",
    "    del train_query_df\n",
    "    del train_query_dataset\n",
    "    del train_query_dataloader\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test | ⏰: 22:39:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [01:07<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 1: 0.6015202531472166, auc: 0.7209660395056977, gauc: 0.7264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 186/186 [01:08<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 2: 0.5984873391928212, auc: 0.7252224748008752, gauc: 0.7264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 185/185 [01:06<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 3: 0.5936238195445086, auc: 0.7311965770525354, gauc: 0.7265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 184/184 [01:05<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 4: 0.5947355258723964, auc: 0.7309812105466249, gauc: 0.7259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183/183 [01:02<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 5: 0.5898863222103953, auc: 0.7356885070646262, gauc: 0.7274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [01:02<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 6: 0.5868191691217842, auc: 0.7394554945166116, gauc: 0.728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 180/180 [00:57<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 7: 0.5867807323733966, auc: 0.740860450730772, gauc: 0.7282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 179/179 [00:53<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 8: 0.585190556402313, auc: 0.7426057222023839, gauc: 0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 178/178 [01:26<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 9: 0.5851454795076606, auc: 0.7444976227776436, gauc: 0.7289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 177/177 [01:21<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 10: 0.5844121846438801, auc: 0.7450366623176242, gauc: 0.7283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 176/176 [01:25<00:00,  2.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 11: 0.5834116547961127, auc: 0.7482898624509837, gauc: 0.7293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 175/175 [01:29<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 12: 0.5828133143697466, auc: 0.7474158766297216, gauc: 0.7284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 173/173 [01:26<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 13: 0.5792231607988391, auc: 0.7518364240506221, gauc: 0.7301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 172/172 [01:15<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 14: 0.575671256974686, auc: 0.755244156452583, gauc: 0.7321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171/171 [01:23<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 15: 0.574589408977687, auc: 0.7567987231737654, gauc: 0.7323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [01:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 16: 0.5744954805163777, auc: 0.7564698981024154, gauc: 0.7324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169/169 [01:30<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 17: 0.5779296667618159, auc: 0.7537078567249336, gauc: 0.7305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 167/167 [01:28<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 18: 0.5750112080288504, auc: 0.756843322216303, gauc: 0.7322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166/166 [01:26<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 19: 0.5739807369838278, auc: 0.7581511221578331, gauc: 0.7329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [01:09<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 20: 0.5789835684227221, auc: 0.7553604257243597, gauc: 0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [01:22<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 21: 0.5725730468587178, auc: 0.7609991321031357, gauc: 0.7346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 163/163 [01:17<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 22: 0.5737305916160163, auc: 0.7595537198377758, gauc: 0.7331\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 162/162 [01:16<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 23: 0.5767630014890506, auc: 0.7572368021887215, gauc: 0.7317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161/161 [01:38<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 24: 0.5719880950376854, auc: 0.7610868832716255, gauc: 0.7344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159/159 [01:22<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 25: 0.5705822190773562, auc: 0.7631484007970598, gauc: 0.7355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 158/158 [01:19<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 26: 0.5715372983036162, auc: 0.7640026430526785, gauc: 0.7356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [01:22<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 27: 0.5712724572913662, auc: 0.7633536241008124, gauc: 0.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [01:10<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 28: 0.5725909863144923, auc: 0.7619593882282615, gauc: 0.7334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 155/155 [01:19<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 29: 0.5725673644773421, auc: 0.7620837799395884, gauc: 0.7339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 154/154 [01:18<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "test loss of user group 30: 0.5708017151851159, auc: 0.7638790657422305, gauc: 0.7353\n"
     ]
    }
   ],
   "source": [
    "# fine-grained test on rr model\n",
    "print(f\"Starting test | ⏰: {time.strftime('%H:%M:%S')}\")\n",
    "model = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "encoder = DeepFM_encoder(num_features, embedding_dim, num_fields)\n",
    "resus_rr = RESUS_RR(num_fields, COLD_USER_THRESHOLD, encoder, model).to(device)\n",
    "checkpoint = torch.load(f\"RESUS_RR-{exp_id}.tar\", map_location=torch.device('cpu'))\n",
    "resus_rr.load_state_dict(checkpoint['model'])\n",
    "\n",
    "resus_rr_test_losses = []\n",
    "resus_rr_test_aucs = []\n",
    "\n",
    "for i in range(1,COLD_USER_THRESHOLD+1,1):\n",
    "    # omit users with <= i interactions.\n",
    "    test_support_set = test_df.groupby('uid',as_index=False).apply(\n",
    "        lambda x: x[:i] if len(x)>i else x[:0])\n",
    "    test_query_set = test_df.groupby('uid',as_index=False).apply(\n",
    "        lambda x: x[i:] if len(x)>i else x[:0])\n",
    "    test_query_dataset = QueryWithSupportDataset(test_query_set,test_support_set,COLD_USER_THRESHOLD)\n",
    "    test_query_dataloader = DataLoader(test_query_dataset, batch_size, shuffle=False, num_workers=8, pin_memory=True)\n",
    "    \n",
    "    test_loss, test_auc = val_query(resus_rr, test_query_dataloader)\n",
    "#     print(f\"test loss of user group {i}: {test_loss}, auc: {test_auc}, gauc: {test_gauc}\")\n",
    "\n",
    "    resus_rr_test_losses += [test_loss]\n",
    "    resus_rr_test_aucs += [test_auc]\n",
    "    resus_rr_test_gaucs += [test_gauc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test losses\n",
      "0.6015202531472166\n",
      "0.5984873391928212\n",
      "0.5936238195445086\n",
      "0.5947355258723964\n",
      "0.5898863222103953\n",
      "0.5868191691217842\n",
      "0.5867807323733966\n",
      "0.585190556402313\n",
      "0.5851454795076606\n",
      "0.5844121846438801\n",
      "0.5834116547961127\n",
      "0.5828133143697466\n",
      "0.5792231607988391\n",
      "0.575671256974686\n",
      "0.574589408977687\n",
      "0.5744954805163777\n",
      "0.5779296667618159\n",
      "0.5750112080288504\n",
      "0.5739807369838278\n",
      "0.5789835684227221\n",
      "0.5725730468587178\n",
      "0.5737305916160163\n",
      "0.5767630014890506\n",
      "0.5719880950376854\n",
      "0.5705822190773562\n",
      "0.5715372983036162\n",
      "0.5712724572913662\n",
      "0.5725909863144923\n",
      "0.5725673644773421\n",
      "0.5708017151851159\n",
      "test aucs\n",
      "0.7209660395056977\n",
      "0.7252224748008752\n",
      "0.7311965770525354\n",
      "0.7309812105466249\n",
      "0.7356885070646262\n",
      "0.7394554945166116\n",
      "0.740860450730772\n",
      "0.7426057222023839\n",
      "0.7444976227776436\n",
      "0.7450366623176242\n",
      "0.7482898624509837\n",
      "0.7474158766297216\n",
      "0.7518364240506221\n",
      "0.755244156452583\n",
      "0.7567987231737654\n",
      "0.7564698981024154\n",
      "0.7537078567249336\n",
      "0.756843322216303\n",
      "0.7581511221578331\n",
      "0.7553604257243597\n",
      "0.7609991321031357\n",
      "0.7595537198377758\n",
      "0.7572368021887215\n",
      "0.7610868832716255\n",
      "0.7631484007970598\n",
      "0.7640026430526785\n",
      "0.7633536241008124\n",
      "0.7619593882282615\n",
      "0.7620837799395884\n",
      "0.7638790657422305\n",
      "test gaucs\n",
      "0.7264\n",
      "0.7264\n",
      "0.7265\n",
      "0.7259\n",
      "0.7274\n",
      "0.728\n",
      "0.7282\n",
      "0.729\n",
      "0.7289\n",
      "0.7283\n",
      "0.7293\n",
      "0.7284\n",
      "0.7301\n",
      "0.7321\n",
      "0.7323\n",
      "0.7324\n",
      "0.7305\n",
      "0.7322\n",
      "0.7329\n",
      "0.731\n",
      "0.7346\n",
      "0.7331\n",
      "0.7317\n",
      "0.7344\n",
      "0.7355\n",
      "0.7356\n",
      "0.7345\n",
      "0.7334\n",
      "0.7339\n",
      "0.7353\n"
     ]
    }
   ],
   "source": [
    "print('test losses')\n",
    "for loss in rr_test_losses:\n",
    "    print(loss)\n",
    "print('test aucs')\n",
    "for loss in rr_test_aucs:\n",
    "    print(loss)\n",
    "print('test gaucs')\n",
    "for loss in rr_test_gaucs:\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
